{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommendation System using SVD\n",
    "\n",
    "This notebook implements a hybrid recommendation system using Singular Value Decomposition (SVD), which combines collaborative filtering and content-based approaches for the educational content recommendation system.\n",
    "\n",
    "## Objectives\n",
    "1. Prepare user-item interaction matrix\n",
    "2. Extract item features for content-based filtering\n",
    "3. Implement collaborative filtering using SVD\n",
    "4. Combine with content-based approach\n",
    "5. Evaluate and tune the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, mean_squared_error\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectures dataset shape: (1021, 6)\n",
      "Merged data shape: (117167, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "lectures_data = pd.read_csv('../data/cleaned/cleaned_lectures.csv')\n",
    "merged_data = pd.read_csv('../data/cleaned/merged_cleaned_data.csv')\n",
    "\n",
    "# Display basic info about the datasets\n",
    "print(f\"Lectures dataset shape: {lectures_data.shape}\")\n",
    "print(f\"Merged data shape: {merged_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subject categories based on ranges of tag values\n",
    "# These mappings are based on TOEIC structure\n",
    "def get_subject_category(tag):\n",
    "    try:\n",
    "        tag_val = float(tag)\n",
    "        if 1 <= tag_val < 23:\n",
    "            return \"Listening Skills\"\n",
    "        elif 23 <= tag_val < 52:\n",
    "            return \"Reading Skills\"\n",
    "        elif 52 <= tag_val < 70:\n",
    "            return \"Speaking Skills\"\n",
    "        elif 70 <= tag_val < 150:\n",
    "            return \"Writing Skills\"\n",
    "        elif 150 <= tag_val < 200:\n",
    "            return \"Test Preparation\"\n",
    "        elif 200 <= tag_val < 300:\n",
    "            return \"Grammar & Vocabulary\"\n",
    "        else:\n",
    "            return \"General\"\n",
    "    except:\n",
    "        return \"General\"\n",
    "\n",
    "# Map part numbers to human-readable names\n",
    "part_names = {\n",
    "    0: \"Introduction\",\n",
    "    1: \"Listening Comprehension\",\n",
    "    2: \"Reading Comprehension\",\n",
    "    3: \"Grammar & Vocabulary\",\n",
    "    4: \"Speaking Assessment\",\n",
    "    5: \"Writing Exercises\",\n",
    "    6: \"Practice Tests\",\n",
    "    7: \"Additional Resources\"\n",
    "}\n",
    "\n",
    "# Apply mappings\n",
    "lectures_data['subject_category'] = lectures_data['tags'].apply(get_subject_category)\n",
    "lectures_data['part_name'] = lectures_data['part'].map(part_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Bundle Features\n",
    "\n",
    "Let's extract bundle information for content-based features in our hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bundles: 8260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karat\\AppData\\Local\\Temp\\ipykernel_17696\\3732587692.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bundle_difficulty = merged_data.groupby('bundle_id').apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_count</th>\n",
       "      <th>part_name</th>\n",
       "      <th>subject_category</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>success_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1</td>\n",
       "      <td>1</td>\n",
       "      <td>1;2;179;181</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening Comprehension</td>\n",
       "      <td>General</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b10</td>\n",
       "      <td>1</td>\n",
       "      <td>17;7;182</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening Comprehension</td>\n",
       "      <td>General</td>\n",
       "      <td>47</td>\n",
       "      <td>0.319149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b100</td>\n",
       "      <td>1</td>\n",
       "      <td>22;2;181</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening Comprehension</td>\n",
       "      <td>General</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b1000</td>\n",
       "      <td>2</td>\n",
       "      <td>24;33;182;183</td>\n",
       "      <td>1</td>\n",
       "      <td>Reading Comprehension</td>\n",
       "      <td>General</td>\n",
       "      <td>46</td>\n",
       "      <td>0.760870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1001</td>\n",
       "      <td>2</td>\n",
       "      <td>34;35;182;183</td>\n",
       "      <td>1</td>\n",
       "      <td>Reading Comprehension</td>\n",
       "      <td>General</td>\n",
       "      <td>12</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bundle_id  part           tags  question_count                part_name  \\\n",
       "0        b1     1    1;2;179;181               1  Listening Comprehension   \n",
       "1       b10     1       17;7;182               1  Listening Comprehension   \n",
       "2      b100     1       22;2;181               1  Listening Comprehension   \n",
       "3     b1000     2  24;33;182;183               1    Reading Comprehension   \n",
       "4     b1001     2  34;35;182;183               1    Reading Comprehension   \n",
       "\n",
       "  subject_category  interaction_count  success_rate  \n",
       "0          General                  6      0.833333  \n",
       "1          General                 47      0.319149  \n",
       "2          General                  7      1.000000  \n",
       "3          General                 46      0.760870  \n",
       "4          General                 12      0.583333  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique bundle information\n",
    "bundle_info = merged_data.groupby('bundle_id').agg({\n",
    "    'part': 'first',\n",
    "    'tags': lambda x: ';'.join(set(str(i) for i in x if pd.notna(i))),\n",
    "    'question_id': lambda x: len(set(x))  # Number of questions in bundle\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "bundle_info.columns = ['bundle_id', 'part', 'tags', 'question_count']\n",
    "\n",
    "# Map part to human-readable names\n",
    "bundle_info['part_name'] = bundle_info['part'].map(part_names)\n",
    "\n",
    "# Create subject category based on tags\n",
    "bundle_info['subject_category'] = bundle_info['tags'].apply(get_subject_category)\n",
    "\n",
    "# Calculate bundle popularity from interaction data\n",
    "bundle_popularity = merged_data['bundle_id'].value_counts().reset_index()\n",
    "bundle_popularity.columns = ['bundle_id', 'interaction_count']\n",
    "\n",
    "# Calculate bundle difficulty from correct answer rates\n",
    "bundle_difficulty = merged_data.groupby('bundle_id').apply(\n",
    "    lambda x: (x['user_answer'] == x['correct_answer']).mean()\n",
    ").reset_index()\n",
    "bundle_difficulty.columns = ['bundle_id', 'success_rate']\n",
    "\n",
    "# Merge all features\n",
    "bundle_features = bundle_info.merge(bundle_popularity, on='bundle_id', how='left')\n",
    "bundle_features = bundle_features.merge(bundle_difficulty, on='bundle_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "bundle_features['interaction_count'] = bundle_features['interaction_count'].fillna(0)\n",
    "bundle_features['success_rate'] = bundle_features['success_rate'].fillna(0.5)\n",
    "\n",
    "print(f\"Total unique bundles: {len(bundle_features)}\")\n",
    "bundle_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create User-Item Interaction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original users: 999\n",
      "Filtered users with 5+ interactions: 881\n",
      "Retained 99.74% of interactions\n"
     ]
    }
   ],
   "source": [
    "# Filter users with minimum interactions for better model performance\n",
    "min_interactions = 5\n",
    "user_counts = merged_data['user_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_interactions].index\n",
    "filtered_data = merged_data[merged_data['user_id'].isin(valid_users)]\n",
    "\n",
    "print(f\"Original users: {merged_data['user_id'].nunique()}\")\n",
    "print(f\"Filtered users with {min_interactions}+ interactions: {len(valid_users)}\")\n",
    "print(f\"Retained {len(filtered_data) / len(merged_data):.2%} of interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-item matrix shape: (881, 8256)\n",
      "Sparsity: 0.9892\n"
     ]
    }
   ],
   "source": [
    "# Create correctness as a measure of interaction quality\n",
    "filtered_data = filtered_data.copy()  # Ensures modifications don't affect original data\n",
    "filtered_data.loc[:, 'correct'] = (filtered_data['user_answer'] == filtered_data['correct_answer']).astype(int)\n",
    "\n",
    "# Aggregate user-item interactions\n",
    "user_item_data = filtered_data.groupby(['user_id', 'bundle_id']).agg(\n",
    "    correctness_rate=('correct', 'mean'),\n",
    "    interaction_count=('correct', 'count'),\n",
    "    avg_time=('elapsed_time', 'mean'),\n",
    "    part=('part', 'first'),\n",
    "    tags=('tags', lambda x: ';'.join(set(str(i) for i in x)))\n",
    ").reset_index()\n",
    "\n",
    "# Create pivot table with interaction count as values\n",
    "user_item_matrix = user_item_data.pivot_table(\n",
    "    index='user_id', \n",
    "    columns='bundle_id', \n",
    "    values='interaction_count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Display user-item matrix characteristics\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Sparsity: {1 - (user_item_matrix > 0).sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create ID Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 881\n",
      "Number of items: 8256\n"
     ]
    }
   ],
   "source": [
    "# Create user and bundle ID mappings\n",
    "user_ids = filtered_data['user_id'].unique()\n",
    "bundle_ids = filtered_data['bundle_id'].unique()\n",
    "\n",
    "# Create user mapping\n",
    "user_map = {user_id: idx for idx, user_id in enumerate(np.unique(user_ids))}\n",
    "reverse_user_map = {idx: user_id for user_id, idx in user_map.items()}\n",
    "\n",
    "# Create item mapping\n",
    "item_map = {bundle_id: idx for idx, bundle_id in enumerate(np.unique(bundle_ids))}\n",
    "reverse_item_map = {idx: bundle_id for bundle_id, idx in item_map.items()}\n",
    "\n",
    "print(f\"Number of users: {len(user_map)}\")\n",
    "print(f\"Number of items: {len(item_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Content Features Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content features matrix shape: (8256, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors for content-based part\n",
    "n_items = len(item_map)\n",
    "\n",
    "# Add difficulty category\n",
    "bundle_features['difficulty_level'] = pd.cut(\n",
    "    bundle_features['success_rate'], \n",
    "    bins=[0, 0.3, 0.7, 1.0], \n",
    "    labels=['Hard', 'Medium', 'Easy']\n",
    ")\n",
    "\n",
    "# One-hot encode features\n",
    "part_dummies = pd.get_dummies(bundle_features['part'], prefix='part')\n",
    "subject_dummies = pd.get_dummies(bundle_features['subject_category'], prefix='subject')\n",
    "difficulty_dummies = pd.get_dummies(bundle_features['difficulty_level'], prefix='difficulty')\n",
    "\n",
    "# Combine feature matrices\n",
    "content_features = pd.concat([part_dummies, subject_dummies, difficulty_dummies], axis=1)\n",
    "\n",
    "# Add bundle_id for mapping\n",
    "content_features = pd.concat([bundle_features[['bundle_id']], content_features], axis=1)\n",
    "\n",
    "# Create a matrix with internal indices\n",
    "# feature_matrix = np.zeros((n_items, content_features.shape[1] - 1))\n",
    "feature_matrix = np.zeros((n_items, content_features.drop(columns=['bundle_id']).shape[1]))\n",
    "\n",
    "# Fill the feature matrix with values\n",
    "for _, row in content_features.iterrows():\n",
    "    bundle_id = row['bundle_id']\n",
    "    if bundle_id in item_map:\n",
    "        idx = item_map[bundle_id]\n",
    "        feature_matrix[idx] = row.values[1:]\n",
    "\n",
    "# Normalize features\n",
    "feature_matrix = normalize(feature_matrix, axis=1, norm='l2')\n",
    "\n",
    "# Compute content similarity\n",
    "content_similarity = cosine_similarity(feature_matrix)\n",
    "\n",
    "print(f\"Content features matrix shape: {feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Ratings Matrix for SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings matrix shape: (881, 8256)\n",
      "Mean rating: 2.0131\n"
     ]
    }
   ],
   "source": [
    "# Create ratings matrix for SVD\n",
    "n_users = len(user_map)\n",
    "n_items = len(item_map)\n",
    "ratings_matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "# Fill with interaction values\n",
    "for _, row in user_item_data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    bundle_id = row['bundle_id']\n",
    "    \n",
    "    if user_id in user_map and bundle_id in item_map:\n",
    "        user_idx = user_map[user_id]\n",
    "        item_idx = item_map[bundle_id]\n",
    "        \n",
    "        # Use interaction_count as the rating\n",
    "        rating = row['interaction_count']\n",
    "        \n",
    "        # Additionally, boost rating if correctness rate is high\n",
    "        if not pd.isna(row['correctness_rate']):\n",
    "            # Scale correctness rate to a small boost (0.1-0.5)\n",
    "            boost = 0.1 + (row['correctness_rate'] * 0.4)\n",
    "            rating = rating * (1 + boost)\n",
    "        \n",
    "        ratings_matrix[user_idx, item_idx] = rating\n",
    "\n",
    "# Calculate mean rating of non-zero entries\n",
    "mean_rating = np.mean(ratings_matrix[ratings_matrix > 0])\n",
    "print(f\"Ratings matrix shape: {ratings_matrix.shape}\")\n",
    "print(f\"Mean rating: {mean_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions: 78,656\n",
      "Training set: 62,924 interactions\n",
      "Test set: 15,732 interactions\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all non-zero entries in the ratings matrix\n",
    "interactions = []\n",
    "for u in range(n_users):\n",
    "    for i in range(n_items):\n",
    "        if ratings_matrix[u, i] > 0:\n",
    "            interactions.append((u, i, ratings_matrix[u, i]))\n",
    "\n",
    "# Split interactions into train and test sets\n",
    "train_interactions, test_interactions = train_test_split(\n",
    "    interactions, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and test matrices\n",
    "train_matrix = np.zeros_like(ratings_matrix)\n",
    "test_matrix = np.zeros_like(ratings_matrix)\n",
    "\n",
    "# Fill train matrix\n",
    "for u, i, r in train_interactions:\n",
    "    train_matrix[u, i] = r\n",
    "\n",
    "# Fill test matrix\n",
    "for u, i, r in test_interactions:\n",
    "    test_matrix[u, i] = r\n",
    "\n",
    "print(f\"Total interactions: {len(interactions):,}\")\n",
    "print(f\"Training set: {len(train_interactions):,} interactions\")\n",
    "print(f\"Test set: {len(test_interactions):,} interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SVD Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD model trained in 0.17 seconds\n",
      "User factors shape: (881, 20)\n",
      "Item factors shape: (8256, 20)\n"
     ]
    }
   ],
   "source": [
    "class SVDHybridRecommender:\n",
    "    \"\"\"SVD-based hybrid recommender system\"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=20, combine_weight=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the recommender.\n",
    "        \n",
    "        Args:\n",
    "            n_factors: Number of latent factors for SVD\n",
    "            combine_weight: Weight for combining CF and CB scores (0-1),\n",
    "                           higher means more weight to CF\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.combine_weight = combine_weight\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.content_similarity = None\n",
    "        self.mean_rating = None\n",
    "    \n",
    "    def fit(self, ratings_matrix, content_similarity=None):\n",
    "        \"\"\"\n",
    "        Train the recommender.\n",
    "        \n",
    "        Args:\n",
    "            ratings_matrix: User-item ratings matrix\n",
    "            content_similarity: Item-item content similarity matrix\n",
    "        \"\"\"\n",
    "        # Store content similarity if provided\n",
    "        self.content_similarity = content_similarity\n",
    "        \n",
    "        # Calculate mean rating\n",
    "        self.mean_rating = np.mean(ratings_matrix[ratings_matrix > 0])\n",
    "        \n",
    "        # Fill missing values with mean\n",
    "        ratings_filled = ratings_matrix.copy()\n",
    "        ratings_filled[ratings_filled == 0] = self.mean_rating\n",
    "        \n",
    "        # Apply SVD\n",
    "        start_time = time.time()\n",
    "        u, sigma, vt = svds(ratings_filled, k=min(self.n_factors, min(ratings_filled.shape) - 1))\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Convert to latent factor matrices\n",
    "        sigma_diag = np.diag(sigma)\n",
    "        self.user_factors = u\n",
    "        self.item_factors = vt.T\n",
    "        \n",
    "        print(f\"SVD model trained in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"User factors shape: {self.user_factors.shape}\")\n",
    "        print(f\"Item factors shape: {self.item_factors.shape}\")\n",
    "    \n",
    "    def predict_rating(self, user_idx, item_idx):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-item pair.\n",
    "        \n",
    "        Args:\n",
    "            user_idx: User index\n",
    "            item_idx: Item index\n",
    "            \n",
    "        Returns:\n",
    "            Predicted rating\n",
    "        \"\"\"\n",
    "        if self.user_factors is None or self.item_factors is None:\n",
    "            return self.mean_rating\n",
    "        \n",
    "        return np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "    \n",
    "    def _get_content_score(self, known_items, item_idx):\n",
    "        \"\"\"\n",
    "        Get content-based score for an item based on items the user has interacted with.\n",
    "        \n",
    "        Args:\n",
    "            known_items: List of item indices the user has interacted with\n",
    "            item_idx: Target item index\n",
    "            \n",
    "        Returns:\n",
    "            Content-based similarity score\n",
    "        \"\"\"\n",
    "        if self.content_similarity is None or len(known_items) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate average similarity to known items\n",
    "        return np.mean([self.content_similarity[item_idx, idx] for idx in known_items])\n",
    "    \n",
    "   # ...existing code...\n",
    "    def get_recommendations(self, user_idx, n=10, exclude_seen=True, known_items=None):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user.\n",
    "        ...\n",
    "        \"\"\"\n",
    "        if self.user_factors is None or self.item_factors is None:\n",
    "            return []\n",
    "        \n",
    "        # Get collaborative filtering scores\n",
    "        cf_scores = np.dot(self.user_factors[user_idx], self.item_factors.T)\n",
    "        \n",
    "        # Initialize final scores\n",
    "        final_scores = cf_scores.copy()\n",
    "        \n",
    "        # Combine with content-based scores if available\n",
    "        if self.content_similarity is not None and known_items is not None and len(known_items) > 0:\n",
    "            for i in range(len(final_scores)):\n",
    "                cb_score = self._get_content_score(known_items, i)\n",
    "                final_scores[i] = (self.combine_weight * cf_scores[i]) + ((1 - self.combine_weight) * cb_score)\n",
    "        \n",
    "        # Get indices of all items\n",
    "        all_items = np.arange(len(final_scores))\n",
    "        \n",
    "        # Exclude seen items if requested\n",
    "        if exclude_seen and known_items is not None and len(known_items) > 0:\n",
    "            mask = np.ones_like(final_scores, dtype=bool)\n",
    "            mask[known_items] = False\n",
    "            all_items = all_items[mask]\n",
    "            final_scores = final_scores[mask]\n",
    "        \n",
    "        # Sort by score and get top N\n",
    "        top_indices = np.argsort(-final_scores)[:n]\n",
    "        \n",
    "        # Get actual item indices\n",
    "        top_items = all_items[top_indices]\n",
    "        top_scores = final_scores[top_indices]\n",
    "        \n",
    "        return list(zip(top_items, top_scores))\n",
    "\n",
    "# Initialize and train the SVD hybrid recommender\n",
    "svd_recommender = SVDHybridRecommender(n_factors=20, combine_weight=0.7)\n",
    "svd_recommender.fit(train_matrix, content_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.5306\n",
      "Calculating precision@10 and recall@10...\n",
      "Precision@10: 0.0194\n",
      "Recall@10: 0.0140\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE on test data\n",
    "def calculate_rmse(model, test_matrix):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Error for a model on the given test data.\n",
    "    \n",
    "    Parameters:\n",
    "    model : Trained recommendation model with a predict_rating(user, item) method\n",
    "    test_matrix : 2D NumPy array or sparse matrix with test ratings\n",
    "\n",
    "    Returns:\n",
    "    float : RMSE value\n",
    "    \"\"\"\n",
    "    non_zero_indices = np.where(test_matrix > 0)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for u, i in zip(non_zero_indices[0], non_zero_indices[1]):\n",
    "        y_true.append(test_matrix[u, i])\n",
    "        y_pred.append(model.predict_rating(u, i))\n",
    "\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Calculate precision and recall at k\n",
    "def calculate_precision_recall_at_k(model, test_matrix, k=10):\n",
    "    \"\"\"\n",
    "    Compute average precision@k and recall@k for all users in test set.\n",
    "\n",
    "    Parameters:\n",
    "    model : Trained recommendation model with get_recommendations(user, n, exclude_seen, known_items)\n",
    "    test_matrix : 2D NumPy array with ground truth test interactions\n",
    "    k : int : Number of top recommendations\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float] : (Average precision@k, Average recall@k)\n",
    "    \"\"\"\n",
    "    precision_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    user_count = 0\n",
    "\n",
    "    for u in range(test_matrix.shape[0]):\n",
    "        test_items = np.where(test_matrix[u] > 0)[0]\n",
    "        if len(test_items) == 0:\n",
    "            continue\n",
    "\n",
    "        known_items = np.where(train_matrix[u] > 0)[0].tolist()\n",
    "        recs = model.get_recommendations(u, n=k, exclude_seen=True, known_items=known_items)\n",
    "        if not recs:\n",
    "            continue\n",
    "\n",
    "        rec_items = [item for item, _ in recs]\n",
    "        hits = len(set(rec_items) & set(test_items))\n",
    "\n",
    "        precision_sum += hits / min(k, len(rec_items))\n",
    "        recall_sum += hits / len(test_items)\n",
    "        user_count += 1\n",
    "\n",
    "    avg_precision = precision_sum / user_count if user_count > 0 else 0.0\n",
    "    avg_recall = recall_sum / user_count if user_count > 0 else 0.0\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = calculate_rmse(svd_recommender, test_matrix)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"Calculating precision@10 and recall@10...\")\n",
    "precision, recall = calculate_precision_recall_at_k(svd_recommender, test_matrix, k=10)\n",
    "print(f\"Precision@10: {precision:.4f}\")\n",
    "print(f\"Recall@10: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Sample Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u100200', 'u100767', 'u101324', 'u101372', 'u102298']\n"
     ]
    }
   ],
   "source": [
    "print(list(user_map.keys())[:5])  # See a few valid user IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user u100200:\n",
      "1. b5 (Score: 0.24)\n",
      "2. b5567 (Score: 0.24)\n",
      "3. b5579 (Score: 0.24)\n",
      "4. b160 (Score: 0.24)\n",
      "5. b192 (Score: 0.24)\n",
      "6. b5584 (Score: 0.24)\n",
      "7. b89 (Score: 0.24)\n",
      "8. b190 (Score: 0.24)\n",
      "9. b173 (Score: 0.24)\n",
      "10. b5577 (Score: 0.24)\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for a user\n",
    "def get_recommendations_for_user(user_id, model, n=10):\n",
    "    \"\"\"Generate recommendations for a user.\"\"\"\n",
    "    # Get user index\n",
    "    if user_id not in user_map:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_map[user_id]\n",
    "    \n",
    "    # Get known items\n",
    "    known_items = np.where(train_matrix[user_idx] > 0)[0]\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recs = model.get_recommendations(user_idx, n=n, exclude_seen=True, known_items=known_items)\n",
    "    \n",
    "    # Convert to original IDs\n",
    "    recommendations = [(reverse_item_map[item], score) for item, score in recs]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Get recommendations for a sample user\n",
    "sample_user_id = \"u100200\"\n",
    "recommendations = get_recommendations_for_user(sample_user_id, svd_recommender, n=10)\n",
    "\n",
    "print(f\"Recommendations for user {sample_user_id}:\")\n",
    "for i, (bundle_id, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {bundle_id} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare with Pure SVD (No Content Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD model trained in 0.16 seconds\n",
      "User factors shape: (881, 20)\n",
      "Item factors shape: (8256, 20)\n",
      "Pure SVD RMSE: 2.5306\n",
      "Calculating precision@10 and recall@10...\n",
      "Pure SVD Precision@10: 0.0439\n",
      "Pure SVD Recall@10: 0.0735\n",
      "\n",
      "Comparison:\n",
      "Method      | RMSE    | Precision@10 | Recall@10\n",
      "------------|---------|--------------|----------\n",
      "Hybrid SVD  | 2.5306  | 0.0194       | 0.0140\n",
      "Pure SVD    | 2.5306  | 0.0439       | 0.0735\n",
      "Improvement | 0.0000  | -0.0246       | -0.0594\n"
     ]
    }
   ],
   "source": [
    "# Train a pure SVD model (without content features)\n",
    "pure_svd = SVDHybridRecommender(n_factors=20, combine_weight=1.0)  # Weight of 1.0 means 100% CF\n",
    "pure_svd.fit(train_matrix)\n",
    "\n",
    "# Evaluate pure SVD\n",
    "pure_rmse = calculate_rmse(pure_svd, test_matrix)\n",
    "print(f\"Pure SVD RMSE: {pure_rmse:.4f}\")\n",
    "\n",
    "print(\"Calculating precision@10 and recall@10...\")\n",
    "pure_precision, pure_recall = calculate_precision_recall_at_k(pure_svd, test_matrix, k=10)\n",
    "print(f\"Pure SVD Precision@10: {pure_precision:.4f}\")\n",
    "print(f\"Pure SVD Recall@10: {pure_recall:.4f}\")\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Method      | RMSE    | Precision@10 | Recall@10\")\n",
    "print(\"------------|---------|--------------|----------\")\n",
    "print(f\"Hybrid SVD  | {rmse:.4f}  | {precision:.4f}       | {recall:.4f}\")\n",
    "print(f\"Pure SVD    | {pure_rmse:.4f}  | {pure_precision:.4f}       | {pure_recall:.4f}\")\n",
    "print(f\"Improvement | {pure_rmse-rmse:.4f}  | {precision-pure_precision:.4f}       | {recall-pure_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/svd_hybrid_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Create a model package with all necessary components\n",
    "model_package = {\n",
    "    'recommender': svd_recommender,\n",
    "    'user_map': user_map,\n",
    "    'item_map': item_map,\n",
    "    'reverse_user_map': reverse_user_map,\n",
    "    'reverse_item_map': reverse_item_map,\n",
    "    'metrics': {\n",
    "        'rmse': rmse,\n",
    "        'precision@10': precision,\n",
    "        'recall@10': recall\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model\n",
    "with open(\"../models/svd_hybrid_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"Model saved to ../models/svd_hybrid_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - EdNet)",
   "language": "python",
   "name": "ednet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
